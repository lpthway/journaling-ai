# ğŸ¤– AI Development Instructions - AI Code Analyzer Project

**CRITICAL**: Read this file EVERY TIME before working on this project!

## ğŸ“‹ Essential First Steps (Every Session)

1. **ALWAYS CHECK PROGRESS FIRST**
   ```bash
   # Run the intelligent phase management script
   ./check-phase-status.sh
   
   # Or manually check progress
   cat progress-tracker.md
   cat project-todo.md
   ```

2. **AUTO-DETECT PHASE TRANSITIONS**
   - The check-phase-status.sh script automatically detects phase completion
   - It provides specific next actions for your current phase
   - It checks task completion percentages across all phases
   - It suggests git workflow actions based on current status

3. **UPDATE SESSION INFO**
   - Add current date/time to progress tracker
   - Note what you're planning to work on
   - Review last session's outcomes

4. **CHECK DEVELOPMENT STANDARDS**
   - Review coding standards below
   - Check current Python/tech stack versions
   - Verify test coverage requirements

4. **PHASE PROGRESSION INTELLIGENCE**
   - Check if current phase is complete and needs transition
   - Auto-detect when to move from Phase 1 â†’ Phase 2 â†’ Phase 3
   - Follow git branching strategy for each phase

## ğŸ”„ Automatic Phase Progression Protocol

### **Phase Transition Detection**
The agent must automatically detect when phases are complete and transition to the next phase:

**Phase 1 â†’ Phase 2 Trigger**:
- All Phase 1 deliverables marked âœ… in `planning/phase-1-foundation.md`
- CLI help system working (`ai-analyze --help`)
- Core tests passing with >80% coverage
- **Auto-Action**: Update project-todo.md status, create Phase 2 branch, begin Claude integration

**Phase 2 â†’ Phase 3 Trigger**:
- All Phase 2 deliverables marked âœ… in `planning/phase-2-core-engine.md`
- End-to-end Python project analysis working
- Session management and resume capability functional
- **Auto-Action**: Update project-todo.md status, create Phase 3 branch, begin multi-LLM work

**Phase Completion Check Command**:
```bash
# Always check phase completion status at session start
grep -c "âœ… COMPLETED" planning/phase-1-foundation.md
# If all major deliverables are âœ…, phase is complete
```

### **Git Workflow for Phases**
- **Phase 1**: `feature/phase-1-foundation` branch
- **Phase 2**: `feature/phase-2-core-engine` branch  
- **Phase 3**: `feature/phase-3-advanced` branch
- **Commits**: Regular commits every 1-2 hours with descriptive messages
- **Merges**: Only merge complete phases to main with version tags

## ğŸ¯ Project Context & Vision

**Goal**: Create a modular, reusable AI-powered code analysis framework that can intelligently analyze any codebase using multiple LLMs with advanced quota management and resume capabilities.

**Current State**: Project initialization phase
**Next Milestone**: Core framework architecture completion

## ğŸ—ï¸ Architecture Analysis Framework

### **System Analysis Approach**
When working on any component, always consider these architectural layers:

**1. Interface Layer** (What users interact with)
- CLI commands and arguments
- Configuration files and validation
- Output formats and reports
- Error messages and help systems

**2. Application Layer** (Business logic and workflows)
- Analysis orchestration and coordination
- Session management and resume logic
- Plugin discovery and registration
- Result aggregation and processing

**3. Domain Layer** (Core business concepts)
- Project type detection algorithms
- Analysis result data models
- Configuration schemas and validation
- Session state and progress tracking

**4. Infrastructure Layer** (External integrations)
- LLM provider integrations (Claude, OpenAI, etc.)
- File system operations and I/O
- Network communications and error handling
- Logging, monitoring, and diagnostics

### **CLI Command Structure Analysis**

```bash
# Core Analysis Commands
ai-analyze                              # Auto-detect and analyze current project
ai-analyze /path/to/project             # Analyze specific project path
ai-analyze --type python_backend        # Force specific project type detection
ai-analyze --config my-config.yaml      # Use custom configuration file

# LLM Provider Selection
ai-analyze --llm claude                 # Use Claude as primary LLM
ai-analyze --llm openai                 # Use OpenAI as primary LLM
ai-analyze --llm copilot                # Use GitHub Copilot Chat as primary LLM
ai-analyze --llm local                  # Use local LLM (Ollama)
ai-analyze --providers claude,openai,copilot    # Set provider preference order

# Analysis Control
ai-analyze --depth 1                    # Shallow analysis (overview only)
ai-analyze --depth 5                    # Deep analysis (full deep-dive)
ai-analyze --focus documentation        # Focus on specific analysis type
ai-analyze --exclude security           # Exclude specific analyzers

# Session Management
ai-analyze --resume                     # Resume interrupted analysis
ai-analyze --session-id abc123          # Resume specific session
ai-analyze --status                     # Check current analysis status
ai-analyze --cleanup                    # Clean up old sessions

# Advanced Features
ai-analyze --watch --on-change          # Continuous monitoring mode
ai-analyze --repos repo1,repo2,repo3    # Multi-repository analysis
ai-analyze --output json               # Change output format
ai-analyze --export /path/to/report     # Export results to specific location

# Development and Debugging
ai-analyze --dry-run                    # Show what would be analyzed
ai-analyze --verbose                    # Verbose logging output
ai-analyze --debug                      # Debug mode with detailed logs
ai-analyze --profile                    # Performance profiling mode
```

### **Component Integration Patterns**

**Plugin Architecture:**
```python
# Analyzer Registration Pattern
@register_analyzer("documentation")
class DocumentationAnalyzer(BaseAnalyzer):
    project_types = [ProjectType.ANY]
    priority = 1

# LLM Provider Registration
@register_llm_provider("claude")
class ClaudeProvider(LLMProvider):
    supports_streaming = True
    cost_per_token = 0.000008
```

**Configuration Hierarchy:**
```
1. Command-line arguments (highest priority)
2. Environment variables (AI_ANALYZER_*)
3. Project-specific config file (.ai-analyzer.yaml)
4. User global config (~/.ai-analyzer/config.yaml)
5. System defaults (lowest priority)
```

**Error Handling Strategy:**
```python
# Graceful degradation pattern
try:
    result = await claude_provider.analyze(prompt)
except QuotaExhaustedError:
    await session.schedule_resume()
    return PartialResult.from_checkpoint()
except NetworkError:
    result = await fallback_provider.analyze(prompt)
except AnalysisError:
    return create_fallback_analysis()
```

### **Technology Stack (Latest & Best)**

**Core Framework:**
- Python 3.11+ (latest stable with performance improvements)
- Pydantic v2.x (data validation and settings)
- Click 8.x (modern CLI framework)
- Rich 13.x (beautiful terminal UI)
- Typer (if Click becomes limiting)

**Configuration & Data:**
- YAML/TOML for configs (human-readable)
- SQLite with async support (simple, reliable)
- Pydantic for configuration validation
- Pathlib for all path operations

**Testing & Quality:**
- pytest with async support
- pytest-cov for coverage (aim for >90%)
- black + isort + ruff (modern Python linting)
- mypy for type checking
- pre-commit hooks

**Advanced Features:**
- asyncio for concurrent operations
- aiofiles for async file operations
- httpx for async HTTP requests
- structlog for structured logging

## ğŸ“ Development Standards

### **Code Organization**
```python
# Every module should have:
"""Module docstring with purpose and usage examples."""

from __future__ import annotations  # Enable modern type hints

import logging
from pathlib import Path
from typing import Any, Optional

# Use structured logging
logger = logging.getLogger(__name__)

class ExampleClass:
    """Clear class docstring with examples."""
    
    def __init__(self, config: Config) -> None:
        """Initialize with type hints."""
        self._config = config
    
    async def process(self, data: str) -> dict[str, Any]:
        """Async by default for I/O operations."""
        logger.info("Processing data", extra={"data_length": len(data)})
        return {"result": "processed"}
```

### **Error Handling**
```python
# Custom exceptions for clarity
class AnalysisError(Exception):
    """Base exception for analysis errors."""
    pass

class QuotaExhaustedError(AnalysisError):
    """Raised when LLM quota is exhausted."""
    pass

# Comprehensive error context
try:
    result = await analyzer.analyze()
except QuotaExhaustedError as e:
    logger.error("Quota exhausted", extra={
        "provider": e.provider,
        "reset_time": e.reset_time,
        "session_id": session.id
    })
    await session.save_resume_point()
    raise
```

### **Configuration Management**
```python
# Use Pydantic for all configuration
from pydantic import BaseSettings, Field

class AnalyzerConfig(BaseSettings):
    """Configuration for AI analyzers."""
    
    max_tokens: int = Field(default=4000, description="Max tokens per request")
    timeout_seconds: int = Field(default=300, description="Request timeout")
    retry_attempts: int = Field(default=3, description="Max retry attempts")
    
    class Config:
        env_prefix = "AI_ANALYZER_"
        env_file = ".env"
```

## ğŸ¤– GitHub Copilot Integration

### **Copilot Provider Implementation**

The AI Code Analyzer integrates with GitHub Copilot Chat via VS Code extension APIs to provide:

1. **In-Editor Analysis**
   - Direct integration with VS Code workspace
   - Real-time code analysis as you type
   - Contextual suggestions within the editor

2. **Copilot Chat Interface**
   - Use `@workspace` commands for project-wide analysis
   - Leverage existing VS Code context and file history
   - Access to current git state and recent changes

3. **VS Code Extension Integration**
   ```python
   # Copilot provider connects to VS Code extension host
   class CopilotProvider(LLMProvider):
       """GitHub Copilot Chat integration via VS Code."""
       
       async def analyze_with_context(self, code: str, context: EditorContext) -> AnalysisResult:
           # Use VS Code extension API to communicate with Copilot
           response = await self.vscode_extension.send_chat_request(
               prompt=f"Analyze this code: {code}",
               context=context.to_copilot_context()
           )
           return self.parse_copilot_response(response)
   ```

4. **Workspace Context Advantages**
   - Automatic access to project structure
   - Integration with existing git workflow
   - No additional API keys required (uses VS Code auth)
   - Leverages file history and change patterns

### **Copilot-Specific Features**

```python
# Enhanced context passing to Copilot
class CopilotContext:
    workspace_files: List[Path]
    recent_changes: GitDiff
    open_editors: List[EditorState]
    current_selection: Optional[CodeSelection]
    
    def to_copilot_prompt(self) -> str:
        """Convert context to Copilot-optimized prompt."""
        return f"""
        @workspace Please analyze this codebase:
        
        Current focus: {self.current_selection.file if self.current_selection else "entire project"}
        Recent changes: {len(self.recent_changes.files)} files modified
        Project structure: {len(self.workspace_files)} total files
        
        Provide analysis focusing on: architecture, code quality, security, performance
        """
```

### **VS Code Extension Requirements**

```json
// package.json dependencies for Copilot integration
{
  "extensionDependencies": ["github.copilot-chat"],
  "contributes": {
    "commands": [
      {
        "command": "ai-analyzer.analyzeWithCopilot",
        "title": "Analyze with Copilot",
        "category": "AI Analyzer"
      }
    ]
  }
}
```

## ğŸ“Š Progress Tracking Protocol

### **After Every Coding Session:**

1. **Update Progress Tracker**
   ```markdown
   ### Session YYYY-MM-DD HH:MM
   **Worked on**: [Component/Feature]
   **Completed**: [What was finished]
   **Challenges**: [Problems encountered]
   **Next**: [What to do next]
   **Files changed**: [List of modified files]
   ```

2. **Update TODO Status**
   ```markdown
   - [x] ~~Task completed~~
   - [ ] Task in progress (50% done - [specific status])
   - [ ] Task pending
   ```

3. **Check Phase Completion & Auto-Transition**
   ```bash
   # Check if current phase is complete
   if grep -q "âœ… COMPLETED" planning/phase-$(current_phase).md; then
       echo "Phase complete - transitioning to next phase"
       # Update project-todo.md phase status
       # Create new git branch for next phase
       # Begin next phase planning review
   fi
   ```

4. **Document Decisions & Architecture**
   ```markdown
   ### Technical Decisions (Date: YYYY-MM-DD)
   - **Decision**: Chose asyncio over threading
   - **Reasoning**: Better for I/O heavy operations
   - **Impact**: All analyzers must be async
   - **Alternatives Considered**: Threading, multiprocessing
   - **Implementation Notes**: Use aiofiles for file I/O
   ```

5. **Git Workflow Management**
   ```bash
   # Regular commits during session
   git add .
   git commit -m "feat: implement [specific feature]
   
   - [Detailed description of changes]
   - [Progress notes]
   - [Any important decisions made]
   
   Progress: Phase X, Sprint Y - [Component] (Z% complete)"
   
   # Phase completion workflow
   if [[ phase_complete ]]; then
       git checkout main
       git merge feature/phase-X-name
       git tag v1.X.0-phaseX-complete
       git push origin main --tags
       git checkout -b feature/phase-$(next)-name
   fi
   ```

6. **Phase Transition Checklist**
   - [ ] All deliverables marked âœ… in current phase planning file
   - [ ] Tests passing with required coverage
   - [ ] Code quality checks passing (black, ruff, mypy)
   - [ ] Documentation updated for completed features
   - [ ] project-todo.md updated with new phase status
   - [ ] Git branch merged and tagged
   - [ ] New phase branch created and planning reviewed

4. **Update Architecture Documentation**
   - Keep architecture.md current with new components
   - Document new interfaces and their relationships
   - Update CLI command reference with new features
   - Add integration patterns and examples

5. **Check Documentation Structure**
   - Ensure all files are in correct `/docs/` subdirectories
   - Update table of contents and cross-references
   - Verify code examples are current and working
   - Maintain consistent documentation style

## ğŸ§ª Testing Strategy

### **Test-Driven Development**
```python
# Write tests FIRST, then implementation
async def test_analyzer_handles_quota_exhaustion():
    """Test that analyzer properly handles quota limits."""
    analyzer = MockAnalyzer()
    analyzer.set_quota_exhausted(True)
    
    with pytest.raises(QuotaExhaustedError):
        await analyzer.analyze("test code")
    
    # Verify resume point was saved
    assert analyzer.session.resume_point is not None
```

### **Coverage Requirements**
- Minimum 90% test coverage
- All public APIs must have tests
- Integration tests for LLM interactions
- Mock external dependencies

## ğŸ”„ Resume Protocol

### **When Resuming Work:**

1. **Environment Setup**
   ```bash
   cd /home/abrasko/Projects/journaling-ai/tools/ai-code-analyzer
   source venv/bin/activate  # If using venv
   python -m pytest  # Verify tests pass
   ```

2. **Context Recovery**
   - Read last session notes
   - Check git status and recent commits
   - Review current branch and pending PRs
   - Verify development environment

3. **Goal Alignment**
   - Confirm current milestone
   - Check if priorities have changed
   - Update task estimates if needed

## ğŸš€ Quality Gates

### **Before Each Commit:**
```bash
# Run quality checks
black .
isort .
ruff check .
mypy .
pytest --cov=src --cov-report=term-missing
```

### **Before Milestone Completion:**
- All tests passing
- Documentation updated
- API documentation current
- Integration tests verified
- Performance benchmarks run

## ğŸ“ File Organization & Documentation Structure

### **Project Root Structure**
```
/tools/ai-code-analyzer/
â”œâ”€â”€ .ai-instructions.md        # THIS FILE - READ FIRST
â”œâ”€â”€ progress-tracker.md        # Session notes and progress
â”œâ”€â”€ project-todo.md           # High-level phase overview
â”œâ”€â”€ architecture.md           # Technical architecture docs
â”œâ”€â”€ README.md                 # User-facing documentation
â”œâ”€â”€ pyproject.toml           # Modern Python project config
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ .gitignore               # Git ignore patterns
â”œâ”€â”€ .pre-commit-config.yaml  # Pre-commit hooks
â”œâ”€â”€ src/ai_analyzer/         # Main source code
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ docs/                    # Structured documentation
â”œâ”€â”€ config/                  # Configuration templates
â”œâ”€â”€ examples/                # Usage examples
â”œâ”€â”€ scripts/                 # Development scripts
â””â”€â”€ planning/                # Development planning docs
```

### **Structured Documentation (`docs/`)**
```
docs/
â”œâ”€â”€ README.md               # Documentation index
â”œâ”€â”€ user-guide/
â”‚   â”œâ”€â”€ installation.md     # Installation instructions
â”‚   â”œâ”€â”€ quick-start.md      # Getting started guide
â”‚   â”œâ”€â”€ configuration.md    # Configuration guide
â”‚   â”œâ”€â”€ usage-examples.md   # Comprehensive examples
â”‚   â””â”€â”€ troubleshooting.md  # Common issues and solutions
â”œâ”€â”€ developer-guide/
â”‚   â”œâ”€â”€ contributing.md     # Contribution guidelines
â”‚   â”œâ”€â”€ development-setup.md # Development environment
â”‚   â”œâ”€â”€ testing.md          # Testing guidelines
â”‚   â”œâ”€â”€ coding-standards.md # Code style and standards
â”‚   â””â”€â”€ release-process.md  # Release management
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ overview.md         # High-level architecture
â”‚   â”œâ”€â”€ data-models.md      # Data structures and schemas
â”‚   â”œâ”€â”€ plugin-system.md    # Plugin architecture
â”‚   â”œâ”€â”€ llm-integration.md  # LLM provider system
â”‚   â””â”€â”€ session-management.md # Resume and state system
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ cli-reference.md    # CLI command reference
â”‚   â”œâ”€â”€ configuration-schema.md # Config file schema
â”‚   â”œâ”€â”€ analyzer-api.md     # Analyzer plugin API
â”‚   â””â”€â”€ llm-provider-api.md # LLM provider API
â””â”€â”€ project-management/
    â”œâ”€â”€ roadmap.md          # Product roadmap
    â”œâ”€â”€ changelog.md        # Version changelog
    â””â”€â”€ known-issues.md     # Known issues and limitations
```

### **Planning Documentation (`planning/`)**
```
planning/
â”œâ”€â”€ phase-1-foundation.md   # Detailed Phase 1 tasks
â”œâ”€â”€ phase-2-core-engine.md  # Detailed Phase 2 tasks
â”œâ”€â”€ phase-3-advanced.md     # Detailed Phase 3 tasks
â”œâ”€â”€ architecture-decisions/ # Architecture Decision Records
â”‚   â”œâ”€â”€ 001-python-async.md
â”‚   â”œâ”€â”€ 002-plugin-system.md
â”‚   â””â”€â”€ 003-configuration.md
â””â”€â”€ research/               # Research and investigation
    â”œâ”€â”€ llm-providers.md    # LLM provider comparison
    â”œâ”€â”€ cli-frameworks.md   # CLI framework evaluation
    â””â”€â”€ testing-strategies.md # Testing approach research
```

## ğŸ¯ Success Metrics

**Phase 1**: Core framework working with basic analysis
**Phase 2**: Multi-LLM support with intelligent failover
**Phase 3**: Advanced features and web UI
**Final**: Production-ready tool used by other developers

## âš ï¸ Critical Reminders

1. **ALWAYS document WHY, not just WHAT**
2. **Type hints are mandatory**
3. **Async by default for I/O operations**
4. **Configuration over hardcoding**
5. **Test before implement**
6. **Update progress after every session**
7. **Use structured logging everywhere**
8. **Handle errors gracefully with context**
9. **NEVER use exclamation marks (!) in terminal commands** - bash history expansion causes issues

---

**Last Updated**: 2025-08-07  
**Current Milestone**: Phase 1 - Core Framework  
**Next Session Goal**: Create project structure and basic CLI
