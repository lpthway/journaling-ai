# backend/alembic/script.py.mako - Migration Template

"""Initial PostgreSQL schema

Revision ID: 063f59923e87
Revises: 
Create Date: 2025-08-04 15:35:48.160959

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '063f59923e87'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('migration_logs',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('migration_type', sa.String(length=50), nullable=False),
    sa.Column('source_file', sa.String(length=255), nullable=False),
    sa.Column('records_processed', sa.Integer(), nullable=False),
    sa.Column('records_successful', sa.Integer(), nullable=False),
    sa.Column('records_failed', sa.Integer(), nullable=False),
    sa.Column('status', sa.String(length=20), nullable=False),
    sa.Column('validation_passed', sa.Boolean(), nullable=False),
    sa.Column('error_details', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('processing_time_seconds', sa.Float(), nullable=True),
    sa.Column('started_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True),
    sa.CheckConstraint("status IN ('pending', 'running', 'completed', 'failed')", name='check_migration_status'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_migration_started', 'migration_logs', ['started_at'], unique=False)
    op.create_index('idx_migration_type_status', 'migration_logs', ['migration_type', 'status'], unique=False)
    op.create_table('psychology_content',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('category', sa.String(length=100), nullable=False),
    sa.Column('subcategory', sa.String(length=100), nullable=True),
    sa.Column('content_type', sa.String(length=50), nullable=False),
    sa.Column('title', sa.String(length=200), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('summary', sa.Text(), nullable=True),
    sa.Column('source', sa.String(length=200), nullable=True),
    sa.Column('author', sa.String(length=100), nullable=True),
    sa.Column('evidence_level', sa.String(length=20), nullable=False),
    sa.Column('tags', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('keywords', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('embedding_vector', postgresql.ARRAY(sa.Float()), nullable=True),
    sa.Column('usage_count', sa.Integer(), nullable=False),
    sa.Column('effectiveness_score', sa.Float(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.CheckConstraint("content_type IN ('technique', 'theory', 'exercise', 'assessment', 'intervention')", name='check_content_type'),
    sa.CheckConstraint("evidence_level IN ('high', 'moderate', 'low', 'theoretical')", name='check_evidence_level'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_psychology_category', 'psychology_content', ['category', 'subcategory'], unique=False)
    op.create_index('idx_psychology_content_search', 'psychology_content', ['content'], unique=False, postgresql_using='gin', postgresql_ops={'content': 'gin_trgm_ops'})
    op.create_index('idx_psychology_effectiveness', 'psychology_content', ['effectiveness_score'], unique=False)
    op.create_index('idx_psychology_keywords', 'psychology_content', ['keywords'], unique=False, postgresql_using='gin')
    op.create_index('idx_psychology_tags', 'psychology_content', ['tags'], unique=False, postgresql_using='gin')
    op.create_index('idx_psychology_type', 'psychology_content', ['content_type'], unique=False)
    op.create_table('users',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('username', sa.String(length=50), nullable=False),
    sa.Column('email', sa.String(length=255), nullable=True),
    sa.Column('full_name', sa.String(length=100), nullable=True),
    sa.Column('password_hash', sa.String(length=255), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('is_verified', sa.Boolean(), nullable=False),
    sa.Column('privacy_level', sa.String(length=20), nullable=False),
    sa.Column('preferences', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('last_login', sa.DateTime(timezone=True), nullable=True),
    sa.CheckConstraint("privacy_level IN ('public', 'friends', 'private')", name='check_privacy_level'),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('email'),
    sa.UniqueConstraint('username')
    )
    op.create_index('idx_users_created_at', 'users', ['created_at'], unique=False)
    op.create_index('idx_users_email', 'users', ['email'], unique=False)
    op.create_index('idx_users_username', 'users', ['username'], unique=False)
    op.create_table('chat_sessions',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('user_id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('title', sa.String(length=200), nullable=True),
    sa.Column('session_type', sa.String(length=50), nullable=False),
    sa.Column('context', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('ai_model', sa.String(length=50), nullable=False),
    sa.Column('system_prompt', sa.Text(), nullable=True),
    sa.Column('temperature', sa.Float(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('message_count', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('last_activity', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.CheckConstraint("session_type IN ('general', 'therapy', 'analysis', 'reflection')", name='check_session_type'),
    sa.CheckConstraint('temperature >= 0.0 AND temperature <= 2.0', name='check_temperature_range'),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_sessions_type', 'chat_sessions', ['session_type'], unique=False)
    op.create_index('idx_sessions_user_activity', 'chat_sessions', ['user_id', 'last_activity'], unique=False)
    op.create_index('idx_sessions_user_created', 'chat_sessions', ['user_id', 'created_at'], unique=False)
    op.create_table('journal_entries',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('user_id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('title', sa.String(length=200), nullable=True),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('raw_content', sa.Text(), nullable=True),
    sa.Column('entry_date', sa.Date(), nullable=False),
    sa.Column('mood_score', sa.Float(), nullable=True),
    sa.Column('word_count', sa.Integer(), nullable=False),
    sa.Column('language', sa.String(length=10), nullable=False),
    sa.Column('sentiment_analysis', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('emotion_analysis', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('topic_analysis', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('psychology_insights', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('embedding_model', sa.String(length=100), nullable=True),
    sa.Column('embedding_vector', postgresql.ARRAY(sa.Float()), nullable=True),
    sa.Column('categories', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('tags', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('is_private', sa.Boolean(), nullable=False),
    sa.Column('is_archived', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.CheckConstraint('mood_score >= 1.0 AND mood_score <= 10.0', name='check_mood_range'),
    sa.CheckConstraint('word_count >= 0', name='check_word_count'),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_entries_categories', 'journal_entries', ['categories'], unique=False, postgresql_using='gin')
    op.create_index('idx_entries_content_search', 'journal_entries', ['content'], unique=False, postgresql_using='gin', postgresql_ops={'content': 'gin_trgm_ops'})
    op.create_index('idx_entries_mood', 'journal_entries', ['mood_score'], unique=False)
    op.create_index('idx_entries_tags', 'journal_entries', ['tags'], unique=False, postgresql_using='gin')
    op.create_index('idx_entries_user_created', 'journal_entries', ['user_id', 'created_at'], unique=False)
    op.create_index('idx_entries_user_date', 'journal_entries', ['user_id', 'entry_date'], unique=False)
    op.create_table('user_analytics',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('user_id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('period_type', sa.String(length=20), nullable=False),
    sa.Column('period_start', sa.Date(), nullable=False),
    sa.Column('period_end', sa.Date(), nullable=False),
    sa.Column('total_entries', sa.Integer(), nullable=False),
    sa.Column('total_words', sa.Integer(), nullable=False),
    sa.Column('avg_mood', sa.Float(), nullable=True),
    sa.Column('sentiment_trends', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('emotion_patterns', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('topic_distribution', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('writing_consistency', sa.Float(), nullable=True),
    sa.Column('peak_writing_hours', postgresql.ARRAY(sa.Integer()), nullable=True),
    sa.Column('most_active_days', postgresql.ARRAY(sa.String()), nullable=True),
    sa.Column('stress_indicators', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('growth_metrics', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('therapy_progress', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('computation_time_ms', sa.Float(), nullable=True),
    sa.Column('data_freshness', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.CheckConstraint("period_type IN ('daily', 'weekly', 'monthly', 'yearly')", name='check_period_type'),
    sa.CheckConstraint('avg_mood >= 1.0 AND avg_mood <= 10.0', name='check_avg_mood_range'),
    sa.CheckConstraint('writing_consistency >= 0.0 AND writing_consistency <= 1.0', name='check_consistency_range'),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('user_id', 'period_type', 'period_start', name='uq_user_analytics_period')
    )
    op.create_index('idx_analytics_computation', 'user_analytics', ['computation_time_ms'], unique=False)
    op.create_index('idx_analytics_freshness', 'user_analytics', ['data_freshness'], unique=False)
    op.create_index('idx_analytics_user_period', 'user_analytics', ['user_id', 'period_type', 'period_start'], unique=False)
    op.create_table('conversations',
    sa.Column('id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('session_id', sa.UUID(as_uuid=False), nullable=False),
    sa.Column('entry_id', sa.UUID(as_uuid=False), nullable=True),
    sa.Column('user_message', sa.Text(), nullable=False),
    sa.Column('ai_response', sa.Text(), nullable=False),
    sa.Column('processing_time_ms', sa.Float(), nullable=True),
    sa.Column('ai_model_used', sa.String(length=50), nullable=False),
    sa.Column('token_usage', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('sources_used', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('citations', postgresql.ARRAY(sa.String()), nullable=True),
    sa.Column('confidence_score', sa.Float(), nullable=True),
    sa.Column('user_feedback', sa.String(length=20), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.CheckConstraint("user_feedback IN ('thumbs_up', 'thumbs_down')", name='check_feedback_values'),
    sa.CheckConstraint('confidence_score >= 0.0 AND confidence_score <= 1.0', name='check_confidence_range'),
    sa.ForeignKeyConstraint(['entry_id'], ['journal_entries.id'], ),
    sa.ForeignKeyConstraint(['session_id'], ['chat_sessions.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_conversations_entry', 'conversations', ['entry_id'], unique=False)
    op.create_index('idx_conversations_processing_time', 'conversations', ['processing_time_ms'], unique=False)
    op.create_index('idx_conversations_session_created', 'conversations', ['session_id', 'created_at'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('idx_conversations_session_created', table_name='conversations')
    op.drop_index('idx_conversations_processing_time', table_name='conversations')
    op.drop_index('idx_conversations_entry', table_name='conversations')
    op.drop_table('conversations')
    op.drop_index('idx_analytics_user_period', table_name='user_analytics')
    op.drop_index('idx_analytics_freshness', table_name='user_analytics')
    op.drop_index('idx_analytics_computation', table_name='user_analytics')
    op.drop_table('user_analytics')
    op.drop_index('idx_entries_user_date', table_name='journal_entries')
    op.drop_index('idx_entries_user_created', table_name='journal_entries')
    op.drop_index('idx_entries_tags', table_name='journal_entries', postgresql_using='gin')
    op.drop_index('idx_entries_mood', table_name='journal_entries')
    op.drop_index('idx_entries_content_search', table_name='journal_entries', postgresql_using='gin', postgresql_ops={'content': 'gin_trgm_ops'})
    op.drop_index('idx_entries_categories', table_name='journal_entries', postgresql_using='gin')
    op.drop_table('journal_entries')
    op.drop_index('idx_sessions_user_created', table_name='chat_sessions')
    op.drop_index('idx_sessions_user_activity', table_name='chat_sessions')
    op.drop_index('idx_sessions_type', table_name='chat_sessions')
    op.drop_table('chat_sessions')
    op.drop_index('idx_users_username', table_name='users')
    op.drop_index('idx_users_email', table_name='users')
    op.drop_index('idx_users_created_at', table_name='users')
    op.drop_table('users')
    op.drop_index('idx_psychology_type', table_name='psychology_content')
    op.drop_index('idx_psychology_tags', table_name='psychology_content', postgresql_using='gin')
    op.drop_index('idx_psychology_keywords', table_name='psychology_content', postgresql_using='gin')
    op.drop_index('idx_psychology_effectiveness', table_name='psychology_content')
    op.drop_index('idx_psychology_content_search', table_name='psychology_content', postgresql_using='gin', postgresql_ops={'content': 'gin_trgm_ops'})
    op.drop_index('idx_psychology_category', table_name='psychology_content')
    op.drop_table('psychology_content')
    op.drop_index('idx_migration_type_status', table_name='migration_logs')
    op.drop_index('idx_migration_started', table_name='migration_logs')
    op.drop_table('migration_logs')
    # ### end Alembic commands ###
